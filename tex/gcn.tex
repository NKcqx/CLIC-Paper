
\section{Selecting Platforms in CLIC}


In CLIC, a logical plan is organized in the form of DAG where the node denotes the logical operator and the edge denotes the operator dependency.
% 先说 opt 有很多平台可用，但存在一个最好的平台，部署到上面可以让plan全局性能最佳。
% 给定一个plan和xx，我们把每个运算符最好的平台看作它的 label。
% 而同一运算符的label不是固定的，而是因 workload, hardware resources 而变化的。
% 因此我们的任务就是根据 xx 的变化，找到运算符的 label 
The platform selection is to find the best platform for each logical operator in a logical plan to achieve the best overall performance.
We take the best platform as the label of the node, i.e. operator, in this way, the selection problem is actually a typical node classification problem.

% 下面这句分两句。第二句先说 GNN 是什么，再说它能直接应用到 topology 上
Such problems with topology as input are usually solved with the Graph Nerual Network(GNN) that can be directly applied to the graph.
Among many GNNs, GCN is considered better at solving the node classification problem because of its ability of capturing neighbor relationships with the convolution kernel.
Compared with the ML-based approach, GCN avoids the loss of structural information due to the need of encoding the topological structure as a vector[];
One more step before feeding the logical plan into GCN is to vectorize the operator, i.e transform the operator object into a numerical vector.
However, as we have disscussed in Section 2.2, the traditional vectorization method is not suitable for CLIC. 
% those drawbacks ? 第二章要不要说呢？说了的话这能直接用 the、those 吗？
Inspired by the word embedding[], we propose a novel feature extraction method called the operator embedding and overcome those drawbacks for the first time.
In 5.1, we give a first look to the the naive encoding approach and discuss its limitations, and then we introduce our novel embedding-based encoding.
In 5.2, we detailed the process of using GCN to select platforms through an example.
In 5.3, we introduce the method of synthesizing training data of the operator embedding and the GCN.

\subsection{Feature Extraction}
As it turns out, many Machine Learning algorithms and almost all Deep Learning Architectures are incapable of processing raw form data like plain text or, in this case, the logical operator. 
% 简单点，说话的方式简单点
They require numbers as inputs to perform any sort of job, be it classification, regression etc. in broad terms. 
Therefore, the operator needs to be encoded as the feature vector before fed into GCN.

When encoding the operator, we consider the following three most influential factors: the operator itself, workload, and hardware resources.

\subsubsection{Operator Encoding}
Operators describe the computation logic of the workflow.
Two logical plans with the same topology can be different because the consisted operators are from two domains.
As examplified in Figure \ref{fig:graph-comparison}, although both of the two plans have the same topology,
\ref{fig:same-graph-sql} describes a SQL query while \ref{fig:same-graph-linear} is a mathmatical calculation.
As a result, they need to be classified to the different execution engines, such as Postgre for Figure \ref{fig:same-graph-sql} and Tensorflow for the other.

\begin{figure}
  \subfigure[SQL Query]{
      \label{fig:same-graph-sql}
      \includegraphics[width=0.45\linewidth]{figures/same-graph-sql.png}
  } 
  \subfigure[Linear Algebra]{
      \label{fig:same-graph-linear}
      \includegraphics[width=0.45\linewidth]{figures/same-graph-linear.png}
  } 
  \caption{Logical Plan Comparison}
  \label{fig:graph-comparison}
\end{figure}

We first follow the traditional approach that using the one-hot encoding to represent the operators.
\textbf{One-Hot Encoding}
The one-hot encoding is used for vectorizing the category features, such as gender or id. 
The vectorization result is a 0-1 vector whose dimension is equal to the number of categories.
The blue part of the feature vector in Figure \ref{fig:feature-vector-onehot} shows the result of the one-hot encoding. 
As we can see that, each row only has one non-zero element which indicates the category of the row.

\begin{figure}
  \subfigure[One-Hot Encoding]{
      \label{fig:feature-vector-onehot}
      \includegraphics[width=0.45\linewidth]{figures/feature-vector-onehot.pdf}
  } 
  \subfigure[Operator Embedding]{
      \label{fig:feature-vector-embedding}
      \includegraphics[width=0.45\linewidth]{figures/feature-vector-embedding.pdf}
  } 
  \caption{Operator Feature Vector}
  \label{fig:feature-vector}
\end{figure}

One-hot encoding is an simple but effective vectorization method and has been widely adopted. 
However, it has two prominent limitations that are especially obvious in CLIC:

\begin{itemize}
  \item 1) Low computational: 
  The dimension of the one-hot vector is equal to the category amounts and only has one non-zero element.
  Therefore it usually resulted in a high-dimensional and sparse vector. 
  Generally speaking, this makes it less computational since the majority of neural network toolkits do not play well with very high-dimensional, sparse vectors [Neural Network Methods in Natural Language Processing, 2017]. 
  This problem becomes more crucial to a cross-platform framework like CLIC since we already face the insufficiant training data problem for a long time.
  \item 2) Imcompatible to new operator insertion: 
  In particular, as a new operator been integrated, the dimension of the one-hot vector will grow in the same time. 
  Figure \ref{fig:feature-vector-onehot} examplified this situation. 
  When the new Flatmap operator (represented by the dashed line) being inserted, all the vectors are grown to a higher dimension.
  The consequnce is that the pre-trained model, GCN for example, needs to be retrained because the dimension of the new vector conflicts with the model's input requirement.
  This problem does not exist in most scenarios, because the dimensions of the input data are usually predefined and will not grow.
  However, as we have narrated in Section 2, a cross-platform system requires the fast evolution, such as enriching the operator set.
  Under these circumstances, the dimension of the one-hot vector will vary frequently, which further lead to the frequent model retraining.
\end{itemize}

In order to overcome the above two limitations, we propose a novel feature extraction method called operator embedding.
% 为了克服以上两个问题，我们提出了新的使用运算符嵌入（operator embedding）向量化运算符的方式。

\textbf{Operator Embedding}
In mathmatic, an embedding is a function $f X -> Y$ that maps a data point X in one space to point Y in another space[wiki]. 
This is the most important preprocess technique in natural language processing (NLP), where the word embedding is a term used for the representation of words for text analysis. 
It is typically in the form of a real-valued vector that encodes the meaning of the word such that the words that are closer in the vector space are expected to be similar in meaning [wiki]. 
For promotion, there also are image embedding, video embedding. 
A representative algorithm for generating word embedding is the CBOW[], whose idea is that words appearing in the same sentence have higher relevance. 
The way CBOW works, in a nutshell, is that it tends to maximize the joint probability of a word and its context words in a sentence. 
Generating the word embedding using CBOW requires a large corpus containing enough sentences as the training dateset to learn the word relationships.

Back to CLIC, we believe that operators also have semantical meanings and are associated to each other.
The semantical meanings, for example, include computing paradigm, \#input/output, etc.
Based on these similarities, we design the operator embedding.

As illustrated in Figure \ref{fig:feature-vector-embedding}, 
the operator embedding is a real-valued continuous vector whose dimension is fixed (hyper parameter) and usually rather smaller than that of the one-hot encoding.
It has two key characteristics that overcome the limitations of the one-hot encoding:
\begin{itemize}
  \item 1) Low-dimensional and Dense. 
  Compare to one-hot encoding's high-dimensional 0-1 vector, 
  the operator embedding is encoded with real-world meanings and therefore only a low-dimensional vector is required to represent an operator.
  This makes it more computational for the following models, and in the meantime, saves a lot of training data.
  \item 2) Compatible to operator evolution. The embedding's dimensions will not grow with the insertion of new operators. 
  As we can see from Figure \ref{fig:feature-vector-embedding}, the newly integrated \textit{tan} operator has consistent dimension with the others.
  The model that takes it as input data does not need to be retrained anymore. 
  This largely supports the fast evolution of CLIC.
\end{itemize}

To generate the operator embedding, we adopt the same ideas in the word embedding that
1) allowing semantically similar operators to have closer (cosine) distance, 
and 2) maximizing the joint probability of an operator and its neighbour operators in a logical plan.
Therefore, we can still use the CBOW algorithm.
We consider the topological ordered logical plan as the "sentence", where each operator is a "word" and its neighbors are the "context words". 
The corpus, i.e. logical plan dataset, are retreived from our synthesized dataset which will be discussed later in Section xx.
Using this corpus as dataset, we finally have the trained operator embeddings visualized in Figure \ref{fig:emb_visual}.
We reduce the dimension to the top-2 dimensions with the highest eigenvalue for the sake of visualization.
The observation is that the operator \textit{tan} is closer to \textit{sin} and far from \textit{union}.
This can be explained semantically as that \textit{tan} and \textit{sin} belong to the same computing paradigm and have the same \#inputs/outputs.
Although the meanings of the embedding are more complex than just paradigm and parameter amounts, 
for CLIC, 
we can simply assume that the operator embedding has successfully portrays the operator.
The power of the embedding will further be revealed when utilizing to train the GCN model, which we will disscuss in Chapter xx.

\begin{figure}[tbh]
  \centering
  \includegraphics[width=0.8\linewidth]{figures/embedding_visual.pdf}
  \caption{Visualization of the trained operator embedding}
  \label{fig:emb_visual}
\end{figure}

One thing we want emphasize is that, 
since the operator used as the input of embedding is only logical,
one can generating an operator's embedding whether it is actually implemented by a framework, say CLIC, or not, 
as long as the corpus they offered contains the operator.
In another word, the operator embedding technique is a general encoding method that is independent to any computing frameworks.

What this merit means to CLIC is that, we can construct a rich operator embedding set as the "dictionary" before we actually implement them.
After that, every time we integrate a new operator, we look up to the dictionary for its embedding. There are two scenario for getting the embedding:
\begin{itemize}
    \item 1) The operator is new to CLIC but not the dictionary. In this case, we retreive its embedding directly from the dictionary.
    \item 2) The operator is "Out-Of-Vocabulary (OOV)". In this case, we need to first synthesize some new logical plans that contain this operator according to its attributes (paradigm, \#input/output, etc.), and then re-generating all of the embeddings.
\end{itemize}


\subsubsection{Hardware Resources}
Hardware resources, including network bandwidth, GPU memory, SSD/HDD, etc. are also important factors affecting platform classification. 
For example, the selection of two different communication model used in ML frameworks, i.e. the parameter server model and the all-reduce model, are effected by the bandwidth and the GPU. 
In general, the parameter server works better if you have a large number of unreliable and not so powerful machine;
All-reduce works better if you have a small amount of fast devices(variance of step time between each device is small) run in a controlled environment with strong connected links. 
The benchmark result in [ML platform Benchmark] and the experiments in Section 2 also support to this. 
Therefore, we also encode the hardware parameters into the feature vector, as shown in the green segment in Figure xx. 
Those features are the same for all operators in a same plan. 
The Figure xx.3 is the classification result of the same plan with the operator platforms are changed from Tensorflow to PyTorch. 
The reason behind it may be that the network card has been upgraded from 9GBS to 200GBS and the communication model of Tensorflow is set to the parameter server by default, and vice versa for PyTorch. 

\subsubsection{Workload}
% 数据分布和 selectivity


put it together, we construct the operator's feature vector as 
Figure xx shows the structure of few sample operator's feature vector. 
It consists of two segments: the operator encoding and the hardware resources (the right green). 
The following will introduce the meaning of these two segments.


\subsection{GCN Intuition}

Graph Convolutional Network is a convolutional neural network that can be directly applied to graphs. 
It applys the convolution kernel to learn the first-order spectral feature, which are followed by activation functions to learn graph representations[42]. 
The convolution kernel learns the spectral feature by inspecting neighboring nodes, below we briefly show the GCN mechanism as an example of the logical plan's node classification process, which is shown in Figure \ref{fig:gcn}.

\begin{figure}
  \centering
  \includegraphics[width=\linewidth]{figures/GCN-new.pdf}
  \caption{Selecting platforms with GCN}
  \label{fig:gcn}
\end{figure}

The input of GCN is a graph with nodes are represented as vectors $\hat{V}$. 
In order to learn the relationship between the current operator (green node) and its neighbors, it first needs to normalize the irregular topology to the matrix. 
This is done by selecting the current operator and its K-1 neighbors (blue nodes) to form the K * |V| size matrix. 
Then the kernel function f(x) = AVG(X) slides across the matrix and generates the resulted vector that is the aggregate of its neighborhood. 
After that, the vector is fed into the nerual network to learn the node's graph representation. 
In our classification case, we add a fully-connected layer at the end of the neural network so that the graph representation is transformed into the label's probability distribution immediately. 
Finally, the one with highest probability is selected as the resulted label.


\subsection{Training GCN}
The dataset used for training were logical plans and labels of operators in the plan. 
The effectiveness of the model depends on the quality of the dataset. 
Therefore, a large number of logical plans are required first. 
In order to get the labels of each plan, we also need to actually run all of its possible physical plans with different workloads and infrastructures, and choose the one with the least execution time. 
But this confronting two chanllenges: 
i) there lacks sufficient real-world logical plans [] and 
ii) it's impossible to run all the possible physical plans since it is an exponential search space. 

For the former problem, we used the synthesis way to generate the traning data. 
We construct a state machine for each computing paradigm respectively to represent its programming model, and synthesize the actual logical plans using the markov chain. 

For the latter problem, we first prune the search space using the technique in []. 
And for the last physical plan, we specified the data source and severa; workloads, and run them to get the labels, then we populate the data by interpolation []. 
At last, we repeate the above steps on variational infrastructures.

We train the GCN model using the above synthesized dataset and Figure xx shows the model accuracy on the collected real-world test set. 
It can be seen that, the model acquires pretty good result on the single computing paradigm. 
However, when dealing with the large graph, especially the hybrid graph, the accuracy drops to nearly 50\%, which is unacceptable.

One of the main problem is the one-hot encoding. 
This encoding method always results in a high-dimensional and sparse vector. 
Generally speaking, this makes it less computational since the majority of neural network toolkits do not play well with very high-dimensional, sparse vectors [Neural Network Methods in Natural Language Processing, 2017]. 
The less computational problem is crucial to such a framework since it already faces the insufficiant data problem. 

Another side-effects of the one-hot encoding is that it is imcompatible to a newly integrated operator, which goes against to our high extensible character. 
In particular, the one-hot encoding needs to increase the dimension of all operator vectors whenver a new one is inserted. 
Figure xx examplified this situation where as the new Flatmap operator (red one) being inserted, all the vectors' dimension are grown. 
Thus the previous model needs to be retrained otherwise the new vector becomes an invalid input due to its higher dimension. 
Even if reserving some empty dimension to stablize the input, the model still cannot properly classify the new operator because the new dimension is orthogonal to the other dimension and the model lacks knowledge about this new dimensions.

In order to overcome the above two problems, we turned our attention to represent the operator using the embedding.


% \begin{figure}
%     \subfigure[One-Hot Encoding]{
%         \label{fig:one-hot}
%         \includegraphics[width=0.3\linewidth]{figures/one-hot.png}
%     } 
%     \subfigure[Operator Embedding]{
%         \label{fig:embedding}
%         \includegraphics[width=0.3\linewidth]{figures/embedding.png}
%     } 
%     \caption{Vectoization Method}
%     \label{fig:encoding}
% \end{figure}


% \subsection{Data Generation}
% \textbf{Generating Operator Embedding}
% In word embedding, because the words in a sentence have certain correlation with its context words, 
% the cost function of training Word Embedding is to maximum the joint probability of the word and its context words. 
% Similarly, we can also reasonably assume that the operators that appear in a same logical plan have certain correlation and use the same cost function. 
% We take the topological-ordered logical plan as the sentence and the logical operators inside are words, 
% then use the same model to train the operator embedding. The results are shown in Figure xx. 
% For the sake of visualization, we just show the top-2 dimensions with the highest eigenvalue. 
% One observation is that operators that are the same computing paradigm and have the same number of inputs and outputs tend to have small cosine distance. 
% What's more, the newly inserted Flatmap operator is also mapped closer to the Map operator instead of the Matmul operator which is probably due to the same reason.

% % TODO: 图之后再调整
% \begin{figure}
%     \subfigure[Operator Embedding]{
%         \label{fig:opt_ebd}
%         \includegraphics[width=0.4\linewidth]{figures/embedding_visual.pdf}
%     } 
%     \subfigure[Platform Embedding]{
%         \label{fig:embedding}
%         \includegraphics[width=0.4\linewidth]{figures/embedding_visual.pdf}
%     } 
%     \caption{Embedding Visualization}
%     \label{fig:embedding_panel}
% \end{figure}

% It's important to emphasize that the operator embedding technique is not bound with CLIC but a general feature extraction method. 
% One can generate an operator embedding whether the operator is supported by CLIC or not as long as there are logical plans that contain this operator.

% As we have mentioned that the operator embedding is compatible for the newly integrated operator. 
% There're two cases when integrating a new operator:
% \begin{itemize}
%     \item [1)]
%     The operator is new to CLIC but not the model, therefore its embedding can be retreived in CLIC directly from the model.
%     \item [2)]
%     The operator is also new to the model, i.e. the "OOV" (out of vocabulary) problem. 
%     In this case, the model needs some new training data to recognize it. 
%     Therefore, we synthesize some new logical plan that contains the operator according to operator's attributes and then incrementally updates the model to generate the embedding.
% \end{itemize}

% Apart from the above operator embedding, 
% we also design and generate the embedding for each physical computing platform. 
% The mathmatical meaning and generation method goes the same. 
% We don't further explain due to limits of space.


% \textbf{Generating Training Data}
% The effectiveness of the GCN strongly depends on the training data. 
% In our case, a large amount of disparate logical plans (data points) together with the best physical platform (labels) of each logical operator in it is required. 
% The logical operator's feature vector is composed of two parts: operator embedding and the global hardware resources. 
% % 1. 举例子说明为什么考虑带宽
% % 2. 把向量图画出来
% % 3. 把这段提前
% The latter is the same for the operators in the same logical plan, including infrastructure's CPU frequency, main memory, GPU memory, network bandwidth, etc. 
% However, there is currently a lack of the Workflow dataset for cross-platform computation, so we borrow the TDGen [] to synthesized datasets separately for each computation mode and on different infrastructures.