\section{Introduction}

Recent progresses in big data and artificial intelligence have significantly enhanced and energized data analytics.  
With analysis task's diverse goals for performance, scalability, and programming efficiency, data processing platforms targeting different domains are emerging and being actively developed. 
For instance, tasks such as data cleaning, filtering and aggregation are generally performed on a DBMS or big data platforms like Spark~\cite{}; 
tasks applied on the graph structure such as relationship discovery are performed by GraphX or Giraph~\cite{}; 
deep learning models are trained by platforms like Tensorflow~\cite{} and Pytorch~\cite{}, to name a few.
According to their specialties and suited data models, a modern data analytics workflow is usually built as a sequence of tasks that are separately executed on multiple platforms~\cite{}. 

Since platforms may have fundamentally different programming
models and performance advantages, building a cross-platform workflow while achieving 
high overall performance not only demands expertise for all platforms, but also needs to develop ad-hoc programs to orchestrate them. 
This gives rise to a spectrum of frameworks that facilitate the development and deployment of cross-platform workflows[]. 
These frameworks support platforms from multiple domains as the back-ends. Users are provided with a transparent view of the underlying platforms and a domain specific language to express workflows. 
With a cost model that considers platform efficiencies for workload and data movement costs, a system selects a platform for each task to optimize the overall performance of the workflow.

Recent years, data scientists have been migrating data analytics tasks to the cloud. The main reasons are as follows. 
First, data sets can be very large in data analytics tasks, while cloud provides conceptually unlimited resources that can support large scale computing and storage. 
This leads to significantly low costs comparing purchasing and maintaining hardwares by users. 
Second, maintaining software environment, e.g., dependencies and platform versions, across a cluster is time-consuming and a misconfigured environment can impact the overall performance[]. 
Cloud is able to effectively manage the environment with virtualization techniques such as container. 
Last but not the least, cloud supports scaling on demand, which avoids resource underutilization with smaller tasks. 
However, existing cross-platform frameworks are not designed for the cloud.

To efficiently run tasks on the cloud, a cross-platform data analytics framework should equip the following capabilities. 
\textit{1) Hardware-Aware Platform Selection} Cloud contains servers with different hardware configurations, while critical factors such as the existence of GPUs could influence the platform selection. For instance, Tensorflow may be chosen for performing a machine learning task on nodes with GPUs, while MPI works better on CPU-only nodes.  Since users may buy different types nodes to deploy their workflows, the platform selection mechanism should consider the hardware configuration to improve its efficiency. However, current approaches assume fixed hardware environment and lack the consideration of hardware in platform selection.  
\textit{2) Agile Platform Integration} Data processing platforms are constantly being developed and updated. While a framework needs to integrate these platforms in time to support new functionalities or higher performance, it makes integrating new platforms with low overhead a desired feature, especially on the cloud that has a variety of users. In current cross-platform systems, integrating a new platform need to change the source code of the framework, which results in rebuilding, testing, and deploying the entire system. Moreover, the model used for platform selection needs to be retrained. All of these involved overheads are nontrivial. 
\textit{3) A Cloud-Native Architecture} To flexibly deploy a data analytics workflow on the cloud, tasks belonging to different platforms need to be decoupled for they may run on nodes with different hardware. Moreover, to utilize the provided fault tolerance and orchestration on the cloud, the framework itself should be implemented as a cloud-native application. These requirements strongly demand a redesign of cross-platform framework to fit the cloud.



To address the above issues, We propose CLIC, a cloud-native cross-platform computing framework that is designed for effectively and efficiently performing data analytics workflows on the cloud.
To cope with the key factors that influence platform selection, i.e., diverse hardware and new platform integration, we adopt the Graph Convolutional Network (GCN) model for selecting platforms. By constructing the workflow as a graph, GCN can be efficient with considering the adjacent tasks in the workflow. [TODO:xxxxx]
CLIC is designed as a cloud-native architecture, where tasks of supported platforms can be directly deployed on cloud and the functional units are decoupled as microservices. In this way, cloud features including liveness detection and self-healing can be utilized to make CLIC a rubost framework for developing and deploying data analytics workflows. 


The contributions of this paper are as follows.
\begin{itemize}
\item [1)]
We propose CLIC, a cloud-native cross-platform framework that runs key functional components as microservices and supports efficiently deploying data analytics tasks of multiple platforms on cloud.
\item [2)]
We introduce deep learning in platform selection, which considers cloud instances with various hardware configurations and facilitates adopting new platforms without retraining the model.
\item [3)]
We implement a prototype of CLIC with multiple integrated platforms and extensively evaluate its performance with a diversity of workloads.

\end{itemize}

The roadmap of this paper is as follows. 
Section 2 introduces the background and motivation. 
Section 3 outlines the overall structure of CLIC, and Section 4 describes the design and implementation. 
Section 5 gives details of GCN implementation. 
Section 6 evaluates the prototype system, and Section 7 concludes the paper.

