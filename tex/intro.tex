\section{Introduction}

Recent progresses in big data and artificial intelligence have significantly enhanced and energized data analytics.  With diverse goals for performance, scalability, and programming efficiency, data processing platforms targeting different domains are emerging and being actively developed. For instance, tasks such as data cleaning, filtering and aggregation are generally
performed on a DBMS or big data platforms like Spark~\cite{}; graph algorithms such as relationship discovery are performed on GraphX or Giraph~\cite{}; deep learning models are trained on platforms like Tensorflow~\cite{} and Pytorch~\cite{}, to name a few.
According to their unique properties and suited data types, a modern data analytics workflow is usually built as a sequence of tasks that are executed on different platforms~\cite{}. For instance, a typical data analytics workflow often consist of data preprocessing tasks on Spark and
a machine learning task to gain insights into the data with Tensorflow or Pytorch. 

Since platforms may have fundamentally different programming
models, building a cross-platform workflow while achieving high
performance not only demands expertise for all platforms, but also
needs to develop ad-hoc programs to orchestrate them. This
gives rise to a spectrum of frameworks that facilitate the development
of cross-platform workflows and help relieve the deployment overhead [2, 17, 22, 37]. 
These frameworks support platforms from multiple domains as the back-ends. Users have a transparent view of the underlying platforms and are provided with an intermediate representation to express workflows. 
By considering workload and platform efficiency, the framework selects platforms for each task in the workflow to optimize the overall performance with a cost model and then deploys the workflow for execution.

In recent years, with the fast development and adoption of cloud, data scientists have been migrating data analytics tasks to the cloud. Cloud is promising for data analytics because the ability of easily scaling can 
help flexibly deploying tasks of different scale, complexity, and performance requirements. Moreover, it relieves developers from maintaining underlying hardware and software environment, leading to faster development and lower cost. However, existing cross-platform frameworks are not designed for the cloud and the critical factors in the cloud. As a result, they are unable to fully exploit the power of cloud for data analytics workflows.

\textit{1) Robust platform selection} Properly selecting the platform that is suitable for the task is the key to improve the performance. Both the traditional heuristic method used in [musketeer, bigdawg] and ML-based method used in [ML based optimizer] lack fully considering of the workflow structure which is important to the overall decision, therefore they could be failed under some circumstance which we will discuss later.

3. Easy to develop and maintain. Although good maintainability is a requirement for all systems, this is of vital important for integrated systems. First of all, the power of such frameworks depend on the number of integrated platforms, however, the more platforms it integrates, the more difficult it is to deploy as each platform has its own dependencies and configurations. Secondly, incremental updating is more common as the framework may integrate a new operator or adapt to a new version of a platform, etc.

1) Agile Platform Integration Data processing
platforms are constantly being developed and updated. While a
framework needs to integrate these platforms in time to support
new functionalities or higher performance, it makes integrating
new platforms with low overhead a desired feature. 2) Environment
Management Platforms are being developed and updated with an
unprecedented speed. Since each update may not only add new
features and programming interfaces but also remove obsolete ones,
developed functional operators can only be executed in specific versions. Moreover, system settings also have a non-trivial impact on
performance [28]. Therefore, environment management is required
to make data science applications be robustly deployed over time.
3) Flexible Scaling According to workloads, tasks in a workflow may
demand different computational resources, from a single node to
a cluster with hundreds of nodes. Since cost models cannot perform well in predicting complex tasks [33], resources for tasks in a
workflow need to be managed and adjusted at runtime. Because of
the complexity and diversity of modern data science applications,
the above functionalities are highly desirable for a cross-platform
framework.



2. High avalibility. Many scientific and business analyzing jobs usually take days, i.e. the long-run jobs. This requires the framework to have fearures like the fault detection and recovery, data backup persistence, etc. i.e. the high availablity. 



However, current federated framework still faces several chanllenges:



\textbf{Lack high availability} The traditional frameworks are born for the complex analyzing jobs, but the vital design flaw is that they lack features like fault tolerance, data backup which are considered neccessary in industry for the long-run jobs. The reason is that they're often designed as the monolith architecture, functional components like optimizer, scheduler and running platforms are coupled, making it hard to implement the above high availability features over one job.

\textbf{Difficult to maintain and continuously develop} Firstly, previous works lack isolated environment management of each platform. Developer need to tackle with the dependencies and various kind of configurations. Secondly, developer need to build, test, and deploy the entire system every time there's a code update because of the monolith architecture, which is inefficient and error-prone.

We propose CLIC, a cloud-native cross-platform computing framework with a DL-based platform selector.

CLIC is a federate framework that integrate multiple platforms from different domain to support building cross-platform workflow platform-free. We distinguish the operator concept to logical operator and physical operator. The former describes only the computation, input/output data and is presented to users. The latter is responsible for the actual execution. CLIC will map the logical operator to physical operator transparently.

In order to select the best platform for logical operators based on jobs and workloads, we introduce a DL-based model. We first present the operator and the platform as vectors of continuous value respectively and then feed them into the DL model. By dedicatedly designing the vector representation, we not only improve the prediction accuracy, but also achieve compatibility of the model with the newly integrated operators and platforms.

We design the framework as a cloud-native architecture and achieve high availability on top of it. More specifically, we decouple the functional units to microservices and containerize them for the better maintenance. By facilitating techniques like livness detection and self-healing, our architecture can execute long-run jobs more stable.


In summary, we make the following contributions:
\begin{itemize}
\item [1)]
We propose a federate framework for building cross-platform computing workflow.
\item [2)]
We design a novel representation of operator and platforms and implement a DL-based model for selecting the platform.
\item [3)]
We discuss the existing issues of previous cross-platform framework and design the cloud-native architecture.
\item [4)]
We implement the CLIC framework and validate the effectiveness through several experiments.
\end{itemize}

The roadmap of this paper is as follows. Section 2 introduces the background and motivation. Section 3 outlines the overall structure of CLIC, and Section 4 describes the design and implementation. Section 5 gives details of GCN implementation. Section 6 evaluates the prototype system, and Section 7 concludes the paper.

